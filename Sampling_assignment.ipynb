{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEw0mLwQhnRQMpatOwum9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kunal-code-u/Sampling_assignment/blob/main/Sampling_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA_wt7GEzSI5",
        "outputId": "afc745d4-6412-4c05-d6af-027596ca1ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-0e18b51d6b9b>:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  samples['Stratified'] = balanced_data.groupby('Class').apply(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling Technique with the Highest Accuracy:\n",
            "Sampling Technique: Random\n",
            "Model: DecisionTree\n",
            "Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load dataset\n",
        "url = \"/content/Creditcard_data.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['Class'])\n",
        "y = data['Class']\n",
        "\n",
        "# Balance the dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Combine balanced dataset\n",
        "balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n",
        "\n",
        "# Sample size calculations\n",
        "confidence_level = 0.95\n",
        "margin_of_error = 0.05\n",
        "p = y_resampled.mean()\n",
        "z = 1.96  # Z-score for 95% confidence\n",
        "\n",
        "# Random Sampling sample size\n",
        "random_sample_size = int((z*2 * p * (1 - p)) / (margin_of_error*2))\n",
        "\n",
        "# Stratified Sampling sample size (proportional to strata variance)\n",
        "strata_weight = balanced_data['Class'].value_counts(normalize=True).std()\n",
        "# Avoid division by zero\n",
        "if strata_weight == 0:\n",
        "    strata_weight = 1  # Assign a default value (e.g., 1 to prevent division by zero)\n",
        "stratified_sample_size = int((z*2 * p * (1 - p)) / ((margin_of_error / strata_weight)*2))\n",
        "\n",
        "# Cluster Sampling sample size (based on clusters)\n",
        "num_clusters = 5\n",
        "cluster_sample_size = int((z*2 * p * (1 - p)) / ((margin_of_error / num_clusters)*2))\n",
        "\n",
        "# Sampling Techniques\n",
        "samples = {}\n",
        "\n",
        "# Random Sampling\n",
        "samples['Random'] = balanced_data.sample(n=random_sample_size, random_state=42)\n",
        "\n",
        "# Stratified Sampling - Exclude the grouping column\n",
        "samples['Stratified'] = balanced_data.groupby('Class').apply(\n",
        "    lambda x: x.sample(int(stratified_sample_size * len(x) / len(balanced_data)), replace=True, random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Systematic Sampling\n",
        "k = len(balanced_data) // random_sample_size\n",
        "samples['Systematic'] = balanced_data.iloc[::k, :].reset_index(drop=True)\n",
        "\n",
        "# Cluster Sampling\n",
        "balanced_data['Cluster'] = pd.cut(balanced_data['Time'], bins=num_clusters, labels=False)\n",
        "selected_clusters = np.random.choice(balanced_data['Cluster'].unique(), size=num_clusters // 2, replace=False)\n",
        "samples['Cluster'] = balanced_data[balanced_data['Cluster'].isin(selected_clusters)].reset_index(drop=True)\n",
        "\n",
        "# Bootstrap Sampling\n",
        "samples['Bootstrap'] = balanced_data.sample(n=random_sample_size, replace=True, random_state=42)\n",
        "\n",
        "# Machine Learning Models\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
        "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for sample_name, sample_data in samples.items():\n",
        "    X_sample = sample_data.drop(columns=['Class', 'Cluster'], errors='ignore')\n",
        "    y_sample = sample_data['Class']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        accuracy = model.score(X_test, y_test)\n",
        "        if sample_name not in results:\n",
        "            results[sample_name] = {}\n",
        "        results[sample_name][model_name] = accuracy\n",
        "\n",
        "# Find the overall best sampling technique and model combination\n",
        "highest_accuracy = 0\n",
        "best_sampling_technique = \"\"\n",
        "best_model = \"\"\n",
        "\n",
        "for sample_name, model_results in results.items():\n",
        "    for model_name, accuracy in model_results.items():\n",
        "        if accuracy > highest_accuracy:\n",
        "            highest_accuracy = accuracy\n",
        "            best_sampling_technique = sample_name\n",
        "            best_model = model_name\n",
        "\n",
        "# Output the best combination\n",
        "print(\"Sampling Technique with the Highest Accuracy:\")\n",
        "print(f\"Sampling Technique: {best_sampling_technique}\")\n",
        "print(f\"Model: {best_model}\")\n",
        "print(f\"Accuracy: {highest_accuracy:.2f}\")"
      ]
    }
  ]
}